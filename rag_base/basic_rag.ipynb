{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEihFnda5ZDc",
        "outputId": "a5087065-cd97-4994-d272-b31009e88768"
      },
      "outputs": [],
      "source": [
        "%pip install pinecone-client\n",
        "%pip install -U sentence-transformers\n",
        "%pip install -U langchain\n",
        "%pip install python-dotenv\n",
        "%pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RM9BZcTe5dV3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Bruno\\Documents\\Universidad\\TACC\\tacc-pukyu-yachay\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "PINECONE_KEY = os.getenv('PINECONE_KEY')\n",
        "HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_341Ep4P6SQj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to C:\\Users\\Bruno\\.cache\\huggingface\\token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from pinecone import Pinecone\n",
        "from huggingface_hub import login\n",
        "\n",
        "pinecone = Pinecone(api_key=PINECONE_KEY)\n",
        "login(token=HUGGINGFACE_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktQmxXNmDVui"
      },
      "source": [
        "#Index Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5UrxqqpD3IAp"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "def create_or_use_index(index_name):\n",
        "  \"\"\"\n",
        "  Checks if the given index exists and creates it if it doesn't.\n",
        "\n",
        "  Args:\n",
        "      index_name: The name of the Pinecone index.\n",
        "\n",
        "  Returns:\n",
        "      The Pinecone index.\n",
        "  \"\"\"\n",
        "  if index_name in pinecone.list_indexes().names():\n",
        "    index = pinecone.Index(index_name)\n",
        "    print(f\"Found existing index: {index_name}\")\n",
        "  else:\n",
        "    print(f\"Creating new index: {index_name}\")\n",
        "    index = pinecone.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(\n",
        "            cloud='aws',\n",
        "            region='us-east-1'\n",
        "        )\n",
        "        )\n",
        "  return index\n",
        "\n",
        "def delete_index(index_name):\n",
        "  \"\"\"\n",
        "  Deletes the given Pinecone index.\n",
        "\n",
        "  Args:\n",
        "      index_name: The name of the Pinecone index.\n",
        "  \"\"\"\n",
        "  if index_name in pinecone.list_indexes().names():\n",
        "    pinecone.delete_index(index_name)\n",
        "    print(f\"Deleted index: {index_name}\")\n",
        "  else:\n",
        "    print(f\"Index {index_name} does not exist.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SvaGRTrrBUk2"
      },
      "outputs": [],
      "source": [
        "index_name = 'pukyu-recetas'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV20TaKG794M",
        "outputId": "d257f78a-3332-4b73-e437-a7b8797cb4e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing index: pukyu-recetas\n"
          ]
        }
      ],
      "source": [
        "index = create_or_use_index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 384,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 260}},\n",
              " 'total_vector_count': 260}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Documents Preproccessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install chardet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import chardet\n",
        "from typing import List, Tuple\n",
        "\n",
        "def preprocess_text_files(folder_path: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Lee y preprocesa archivos de texto de una carpeta, manejando diferentes codificaciones.\n",
        "\n",
        "    Args:\n",
        "    folder_path (str): Ruta a la carpeta que contiene los archivos de texto.\n",
        "\n",
        "    Returns:\n",
        "    List[Tuple[str, str]]: Una lista de tuplas, cada una conteniendo el nombre del archivo\n",
        "    y su contenido como una cadena de texto.\n",
        "    \"\"\"\n",
        "    processed_files = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            \n",
        "            # Leer el archivo en modo binario para detectar la codificación\n",
        "            with open(file_path, 'rb') as file:\n",
        "                raw_data = file.read()\n",
        "            \n",
        "            # Detectar la codificación\n",
        "            detected = chardet.detect(raw_data)\n",
        "            encoding = detected['encoding']\n",
        "\n",
        "            # Leer el archivo con la codificación detectada\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding=encoding) as file:\n",
        "                    content = file.read()\n",
        "                \n",
        "                # Realizar cualquier limpieza o normalización adicional aquí\n",
        "                # Por ejemplo:\n",
        "                content = content.replace('\\r\\n', '\\n')  # Normalizar saltos de línea\n",
        "                content = ' '.join(content.split())  # Eliminar espacios múltiples\n",
        "                \n",
        "                processed_files.append((filename, content))\n",
        "                print(f\"Procesado: {filename} (Codificación: {encoding})\")\n",
        "            except UnicodeDecodeError:\n",
        "                print(f\"Error al decodificar: {filename}. Intentando con UTF-8.\")\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                        content = file.read()\n",
        "                    processed_files.append((filename, content))\n",
        "                    print(f\"Procesado con UTF-8: {filename}\")\n",
        "                except UnicodeDecodeError:\n",
        "                    print(f\"No se pudo procesar: {filename}\")\n",
        "\n",
        "    return processed_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Procesado: Cau_Cau_de_Pollo.txt (Codificación: utf-8)\n",
            "Procesado: Cañitas_fritas_rellenas_de_crema.txt (Codificación: utf-8)\n",
            "Procesado: Cebiche_de_Pollo.txt (Codificación: utf-8)\n",
            "Procesado: Champiñones_al_ajillo.txt (Codificación: utf-8)\n",
            "Procesado: Chanfainita.txt (Codificación: utf-8)\n",
            "Procesado: Charquican.txt (Codificación: utf-8)\n",
            "Procesado: Chicharrón_de_Pescado.txt (Codificación: utf-8)\n",
            "Procesado: Chili_con_carne.txt (Codificación: utf-8)\n",
            "Procesado: Chimichanga_dulce.txt (Codificación: utf-8)\n",
            "Procesado: Chipirones_a_la_plancha.txt (Codificación: utf-8)\n",
            "Procesado: Chipirones_encebollados.txt (Codificación: utf-8)\n",
            "Procesado: Choquitos_en_su_tinta.txt (Codificación: utf-8)\n",
            "Procesado: Chorizos_al_vino.txt (Codificación: utf-8)\n",
            "Procesado: Chorizos_a_la_sidra.txt (Codificación: utf-8)\n",
            "Procesado: Chuleta_de_cerdo_con_piperrada.txt (Codificación: utf-8)\n",
            "Procesado: Chupe_de_Camaroncito_Chino.txt (Codificación: utf-8)\n",
            "Procesado: Churros.txt (Codificación: utf-8)\n",
            "Procesado: Cochifrito.txt (Codificación: utf-8)\n",
            "Procesado: Cochinillo_asado_a_la_segoviana.txt (Codificación: utf-8)\n",
            "Procesado: Cocido_gallego.txt (Codificación: utf-8)\n",
            "Procesado: Cocochas_de_merluza_al_pil_pil.txt (Codificación: utf-8)\n",
            "Procesado: Codillo_asado_a_la_cerveza.txt (Codificación: utf-8)\n",
            "Procesado: Cogote_de_merluza_a_la_donostiarra.txt (Codificación: utf-8)\n",
            "Procesado: Conejo_al_ajo_cabañil.txt (Codificación: utf-8)\n",
            "Procesado: Conejo_a_la_cazadora.txt (Codificación: utf-8)\n",
            "Procesado: Congrio_con_fideos.txt (Codificación: utf-8)\n",
            "Procesado: Costillas_de_matanza.txt (Codificación: utf-8)\n",
            "Procesado: Costilla_asada.txt (Codificación: utf-8)\n",
            "Procesado: Costilla_de_cerdo_al_horno.txt (Codificación: utf-8)\n",
            "Procesado: Coulant_de_chocolate.txt (Codificación: utf-8)\n",
            "Procesado: Crema_catalana.txt (Codificación: utf-8)\n",
            "Procesado: Crema_de_calabaza.txt (Codificación: utf-8)\n",
            "Procesado: Crema_de_zanahoria.txt (Codificación: utf-8)\n",
            "Procesado: Crema_pastelera.txt (Codificación: utf-8)\n",
            "Procesado: Cristinas_con_nata_y_fresas.txt (Codificación: utf-8)\n",
            "Procesado: Croquetas_de_jamón.txt (Codificación: utf-8)\n",
            "Procesado: Donuts.txt (Codificación: utf-8)\n",
            "Procesado: Dorada_al_horno.txt (Codificación: utf-8)\n",
            "Procesado: Dorada_a_la_bilbaína.txt (Codificación: utf-8)\n",
            "Procesado: Dulce_de_leche_fácil.txt (Codificación: utf-8)\n",
            "Procesado: Empedrat.txt (Codificación: utf-8)\n",
            "Procesado: Ensaimada_mallorquina.txt (Codificación: utf-8)\n",
            "Procesado: Ensalada_Americana.txt (Codificación: utf-8)\n",
            "Procesado: Ensalada_campera.txt (Codificación: utf-8)\n",
            "Procesado: Ensalada_Caprese.txt (Codificación: utf-8)\n",
            "Procesado: Ensalada_César.txt (Codificación: utf-8)\n",
            "Procesado: Ensalada_de_ahumados_y_encurtidos.txt (Codificación: utf-8)\n",
            "Procesado: Ensalada_de_cherrys,_cebolleta_y_trigueros.txt (Codificación: utf-8)\n",
            "Procesado: Ensalada_de_langostinos_y_aguacate.txt (Codificación: utf-8)\n",
            "Procesado: Ensalada_de_pasta.txt (Codificación: utf-8)\n",
            "Procesado: Ensalada_de_pasta_y_salmón_ahumado.txt (Codificación: utf-8)\n",
            "Procesado: Ensalada_de_pimientos_asados.txt (Codificación: utf-8)\n",
            "Procesado: Ensalada_de_pimientos_con_ventresca.txt (Codificación: utf-8)\n",
            "Procesado: Ensaladilla_rusa.txt (Codificación: utf-8)\n",
            "Procesado: Escabeche_de_Pollo.txt (Codificación: utf-8)\n",
            "Procesado: Escalope_a_la_milanesa.txt (Codificación: utf-8)\n",
            "Procesado: Escalopines_al_Jerez.txt (Codificación: utf-8)\n",
            "Procesado: Espaguetis_a_la_carbonara.txt (Codificación: utf-8)\n",
            "Procesado: Espaguetis_a_la_napolitana.txt (Codificación: utf-8)\n",
            "Procesado: Espárragos_con_salsa_tártara.txt (Codificación: utf-8)\n",
            "Procesado: Espárragos_trigueros_con_panceta.txt (Codificación: utf-8)\n",
            "Procesado: Esqueixada.txt (Codificación: utf-8)\n",
            "Procesado: Estofado_de_pollo.txt (Codificación: utf-8)\n",
            "Procesado: Fabada_asturiana.txt (Codificación: utf-8)\n",
            "Procesado: Fabes_con_almejas.txt (Codificación: utf-8)\n",
            "Procesado: Fideos_con_almejas.txt (Codificación: utf-8)\n",
            "Procesado: Fideuá.txt (Codificación: utf-8)\n",
            "Procesado: Filete_Brasa_con_Arroz_Primavera.txt (Codificación: utf-8)\n",
            "Procesado: jiaco olluco.txt (Codificación: Windows-1252)\n",
            "Procesado: quy_receta_Ajiaco_de_Papa.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Ají_de_Gallina.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Alubias_estofadas_con_setas.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Arroz_a_banda.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Arroz_a_la_Jardinera_con_Pescado.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Arroz_caldero_murciano.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Arroz_con_bacalao.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Arroz_con_bogavante.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Arroz_con_Chancho_Lambayecano.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Arroz_con_leche.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Arroz_con_pollo.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Arroz_Tapado.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Berenjenas_rellenas_de_carne.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Brownies.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Bígaros_cocidos.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Caldeirada_de_raya.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Caldo_Verde.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Callos_a_la_madrileña.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Carne_ó_caldeiro.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Cau_Cau_de_Pollo.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Chipirones_a_la_plancha.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Churros.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Cocido_gallego.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Codillo_asado_a_la_cerveza.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Congrio_con_fideos.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Costilla_de_cerdo_al_horno.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Crema_catalana.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Crema_de_zanahoria.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Ensalada_de_pasta.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Ensalada_de_pimientos_asados.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Ensalada_de_pimientos_con_ventresca.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Espaguetis_a_la_napolitana.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Espárragos_trigueros_con_panceta.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Fabada_asturiana.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Fideos_con_almejas.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Gulas_con_gambas_al_ajillo.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Judías_verdes_con_chorizo.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Macedonia_de_frutas.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Mousse_de_chocolate.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Navajas_a_la_plancha.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Panna_cotta.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Patatas_a_la_riojana.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Pavo_Navideño_Relleno.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Pollo_al_Horno.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Pollo_al_Horno_Maggi_Brasa.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Pollo_a_la_Cerveza.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Pollo_guisado.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Pulpo_a_la_gallega.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Pulpo_encebollado.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Pulpo_á_feira.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Queso_frito.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Revuelto_de_bacalao_y_patata.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Richada_de_Forcarei.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Roscón_de_Pascua.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Salsa_Boloñesa.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Salsa_pico_de_gallo.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Seco_de_Pollo.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Sopa_de_fideos.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Taboulé_de_quinoa.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Tallarines_a_la_marinera.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Tallarines_Verdes_con_Maggi_Brasa.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Tarta_San_Marcos.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Terrina_de_pulpo.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Tocinillo_de_cielo.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Tortitas_de_nata.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Truchas_a_la_navarra.txt (Codificación: utf-8)\n",
            "Procesado: quy_receta_Vieiras_a_la_gallega.txt (Codificación: utf-8)\n",
            "Error al decodificar: receta_Ajiaco_de_Papa.txt. Intentando con UTF-8.\n",
            "No se pudo procesar: receta_Ajiaco_de_Papa.txt\n",
            "Procesado: receta_Ají_de_Gallina.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Albóndigas_con_tomate.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Alcachofas_con_jamón.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Alcachofas_fritas.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_All_i_pebre.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Almejas_al_Albariño.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Almejas_a_la_marinera.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Almejas_en_salsa_verde.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Almejas_à_Bulhão_Pato.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Alubias_con_chorizo.txt (Codificación: None)\n",
            "Procesado: receta_Alubias_estofadas_con_setas.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Aros_de_cebolla_fritos.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Arroz_a_banda.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Arroz_a_la_Jardinera_con_Pescado.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Arroz_a_la_milanesa.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Arroz_caldero_murciano.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Arroz_caldoso_con_nécoras.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Arroz_con_bacalao.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Arroz_con_berberechos.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Arroz_con_bogavante.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Arroz_con_Carne.txt (Codificación: Windows-1252)\n",
            "Error al decodificar: receta_Arroz_con_Chancho_Lambayecano.txt. Intentando con UTF-8.\n",
            "No se pudo procesar: receta_Arroz_con_Chancho_Lambayecano.txt\n",
            "Procesado: receta_Arroz_con_conejo.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Arroz_con_leche.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Arroz_con_Mariscos.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Arroz_con_Pollo.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Arroz_negro.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Arroz_pilaf.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Arroz_Tapado.txt (Codificación: Windows-1254)\n",
            "Procesado: receta_Asadura_de_lechazo.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Atún_encebollado.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Bacalao_a_la_gallega.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Bacalao_a_la_portuguesa.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Bacalao_con_fritada_de_cebolla_y_pimientos.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Bacalao_á_Bras.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Banana_split.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Banda_de_hojaldre_y_manzana.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Berberechos_al_vapor.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Berenjenas_rellenas_de_carne.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Bica_mantecada.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Bollos_suízos.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Bonito_con_salsa_de_tomate_y_ñoras.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Boquerones_en_vinagre.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Brazo_de_gitano.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Brownies.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Bulgogi.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Buñuelos_de_bacalao_y_patata.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Bígaros_cocidos.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Cachopo.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Cake_de_zanahoria_y_nueces.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Calamares_al_ajillo.txt (Codificación: ISO-8859-1)\n",
            "Procesado: receta_Calamares_rellenos_de_carne.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Caldeirada_de_raya.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Caldereta_de_cordero.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Caldereta_de_pescados_y_mariscos.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Caldo_de_Gallina.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Caldo_de_Morón.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Caldo_gallego.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Caldo_Verde.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Callos_a_la_madrileña.txt (Codificación: Windows-1252)\n",
            "Error al decodificar: receta_Callos_con_garbanzos.txt. Intentando con UTF-8.\n",
            "No se pudo procesar: receta_Callos_con_garbanzos.txt\n",
            "Procesado: receta_Canelones_de_San_Esteban.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Cangrejos_a_la_riojana.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Capón_estofado.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Carne_ó_caldeiro.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Carrilleras_de_ternera_al_vino_tinto.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Filloas.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Flamenquines.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Flan_de_café.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Flan_de_huevo.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Flores_de_carnaval.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Foie_con_boletus.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Foie_con_manzana.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Frejoles.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Fritura_a_la_andaluza.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Gambas_al_ajillo.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Gambas_a_la_plancha.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Garbanzos_con_chorizo_y_lacón.txt (Codificación: None)\n",
            "Procesado: receta_Gazpacho_andaluz.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Guacamole.txt (Codificación: Windows-1252)\n",
            "Error al decodificar: receta_Guiso_de_Carne.txt. Intentando con UTF-8.\n",
            "No se pudo procesar: receta_Guiso_de_Carne.txt\n",
            "Error al decodificar: receta_Guiso_de_Pollo.txt. Intentando con UTF-8.\n",
            "No se pudo procesar: receta_Guiso_de_Pollo.txt\n",
            "Procesado: receta_Guiso_de_Trigo_con_Pollo.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Gulas_al_ajillo.txt (Codificación: ascii)\n",
            "Procesado: receta_Gulas_con_gambas_al_ajillo.txt (Codificación: Windows-1252)\n",
            "Error al decodificar: receta_Huatia_con_Camote_Sancochado.txt. Intentando con UTF-8.\n",
            "No se pudo procesar: receta_Huatia_con_Camote_Sancochado.txt\n",
            "Procesado: receta_Huevos_fritos_con_jamón.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Huevos_rellenos_de_atún_y_aceitunas.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Huevos_revueltos_con_trigueros_y_gambas.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Hummus.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Hígado_encebollado.txt (Codificación: Windows-1252)\n",
            "Error al decodificar: receta_Inchicapi_de_Gallina.txt. Intentando con UTF-8.\n",
            "No se pudo procesar: receta_Inchicapi_de_Gallina.txt\n",
            "Procesado: receta_Jamón_asado.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Jarrete_estofado_al_vino_blanco.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Judías_verdes_con_chorizo.txt (Codificación: Windows-1254)\n",
            "Procesado: receta_Judías_verdes_con_tomate.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Lacón_con_grelos.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Lamprea_a_la_bordelesa.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Langostinos_al_horno.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Langostinos_al_whisky.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Langostinos_cocidos.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Larpeira.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Lechazo_asado.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Leche_frita.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Lenguado_meunière.txt (Codificación: None)\n",
            "Error al decodificar: receta_Lentejas_estofadas.txt. Intentando con UTF-8.\n",
            "No se pudo procesar: receta_Lentejas_estofadas.txt\n",
            "Error al decodificar: receta_Locro_con_Pollo.txt. Intentando con UTF-8.\n",
            "No se pudo procesar: receta_Locro_con_Pollo.txt\n",
            "Procesado: receta_Lubina_a_la_espalda.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Lubina_a_la_sal.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Macarrones_con_carne.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Macarrones_con_tomate_y_ricotta.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Macedonia_de_frutas.txt (Codificación: Windows-1254)\n",
            "Procesado: receta_Madeleines.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Magdalenas.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Marmitako_de_bonito_del_norte.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Marmitako_de_caballa.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Mejillones_al_vapor.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Mejillones_a_la_marinera.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Mejillones_a_la_vinagreta.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Mejillones_en_escabeche.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Mejillones_tigres.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Melón_con_jamón.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Merluza_a_la_cazuela.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Merluza_a_la_gallega.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Merluza_a_la_romana.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Mermelada_de_higos.txt (Codificación: Windows-1252)\n",
            "Error al decodificar: receta_Migas_de_pastor.txt. Intentando con UTF-8.\n",
            "No se pudo procesar: receta_Migas_de_pastor.txt\n",
            "Procesado: receta_Moules_-_Frites.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Mousse_de_chocolate.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Mousse_de_limón.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Mousse_de_mango.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Mousse_de_queso_Cebreiro.txt (Codificación: Windows-1252)\n",
            "Procesado: receta_Nachos_con_queso,_chile_y_cilantro.txt (Codificación: Windows-1252)\n"
          ]
        }
      ],
      "source": [
        "recepies_path = '../recetas_quy/'\n",
        "\n",
        "preprocessed_files = preprocess_text_files(recepies_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aSrTRXLDeNy"
      },
      "source": [
        "#Embeddings creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2bkSEFKkDRiw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List, Tuple, Dict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def process_documents_and_create_embeddings(\n",
        "    processed_documents: List[Tuple[str, str]],\n",
        "    model_name: str = 'all-MiniLM-L6-v2',\n",
        "    chunk_size: int = 100,\n",
        "    chunk_overlap: int = 20\n",
        ") -> Tuple[List[Dict[str, str]], List[List[float]]]:\n",
        "    \"\"\"\n",
        "    Procesa documentos preprocesados, los divide en chunks y crea embeddings.\n",
        "\n",
        "    Args:\n",
        "    processed_documents (List[Tuple[str, str]]): Lista de tuplas (nombre_archivo, contenido).\n",
        "    model_name (str): Nombre del modelo de SentenceTransformer a utilizar.\n",
        "    chunk_size (int): Tamaño máximo de cada chunk de texto.\n",
        "    chunk_overlap (int): Superposición entre chunks consecutivos.\n",
        "\n",
        "    Returns:\n",
        "    Tuple[List[Dict[str, str]], List[List[float]]]: Una tupla conteniendo la lista de chunks \n",
        "    (con metadatos) y la lista de sus embeddings correspondientes.\n",
        "    \"\"\"\n",
        "    # Inicializar el modelo de embedding\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    # Inicializar el divisor de texto\n",
        "    #text_splitter = RecursiveCharacterTextSplitter(\n",
        "    #    chunk_size=chunk_size,\n",
        "    #    chunk_overlap=chunk_overlap\n",
        "    #)\n",
        "\n",
        "    #all_chunks = []\n",
        "    all_texts = []\n",
        "\n",
        "    # Leer y procesar cada archivo en la carpeta\n",
        "    for filename, content in processed_documents:\n",
        "        #chunks = text_splitter.split_text(content)\n",
        "        #for chunk in chunks:\n",
        "        #    all_texts.append({\n",
        "        #        \"text\": chunk,\n",
        "        #        \"source\": filename\n",
        "        #    })\n",
        "        all_texts.append({\n",
        "            \"text\": content,\n",
        "            \"source\": filename\n",
        "        })\n",
        "\n",
        "    # Extraer solo el texto de los chunks para crear embeddings\n",
        "    texts = [text[\"text\"] for text in all_texts]\n",
        "\n",
        "    # Crear embeddings para todos los chunks\n",
        "    embeddings = model.encode(texts)\n",
        "\n",
        "    return all_texts, embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_texts, embeddings = process_documents_and_create_embeddings(preprocessed_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Uploading embeddings to Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "def upload_to_pinecone(\n",
        "    texts: List[str],\n",
        "    embeddings: List[List[float]],\n",
        "    batch_size: int = 100\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Sube chunks de texto y sus embeddings correspondientes a Pinecone.\n",
        "\n",
        "    Args:\n",
        "    chunks (List[str]): Lista de chunks de texto.\n",
        "    embeddings (List[List[float]]): Lista de embeddings correspondientes a los chunks.\n",
        "    batch_size (int): Tamaño del lote para las operaciones de upsert.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Verificar que el número de chunks y embeddings coincida\n",
        "    if len(texts) != len(embeddings):\n",
        "        raise ValueError(\"El número de chunks y embeddings debe ser el mismo.\")\n",
        "\n",
        "    # Preparar los datos para la subida\n",
        "    total_vectors = len(texts)\n",
        "    for i in range(0, total_vectors, batch_size):\n",
        "        batch_chunks = texts[i:i+batch_size]\n",
        "        batch_embeddings = embeddings[i:i+batch_size]\n",
        "        \n",
        "        # Crear IDs únicos para cada vector\n",
        "        ids = [str(uuid.uuid4()) for _ in range(len(batch_chunks))]\n",
        "        \n",
        "        # Preparar los vectores para el upsert\n",
        "        vectors_to_upsert = list(zip(ids, batch_embeddings, [{\"text\": chunk} for chunk in batch_chunks]))\n",
        "        \n",
        "        # Realizar el upsert\n",
        "        index.upsert(vectors=vectors_to_upsert)\n",
        "        \n",
        "        print(f\"Subidos {i+len(batch_chunks)} de {total_vectors} vectores\")\n",
        "\n",
        "    print(\"Subida completada.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subidos 100 de 260 vectores\n",
            "Subidos 200 de 260 vectores\n",
            "Subidos 260 de 260 vectores\n",
            "Subida completada.\n"
          ]
        }
      ],
      "source": [
        "texts = [file['text'] for file in all_texts]\n",
        "upload_to_pinecone(texts, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Testing the RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_llm_response(query: str, chunks: List[Dict[str, float]]) -> str:\n",
        "    \"\"\"\n",
        "    Simula la respuesta de un LLM basada en la consulta y los chunks recuperados.\n",
        "    \n",
        "    En un escenario real, esta función sería reemplazada por una llamada al modelo llama2-7b-hf.\n",
        "    \"\"\"\n",
        "    # Esta es una simulación muy básica\n",
        "    combined_context = \" \".join([chunk for chunk, _ in chunks])\n",
        "    response = f\"Basándome en la consulta '{query}' y el contexto proporcionado, \"\n",
        "    response += f\"puedo decir que la información relevante incluye {len(chunks)} fragmentos de texto. \"\n",
        "    response += f\"El contexto total tiene {len(combined_context)} caracteres. \"\n",
        "    response += \"Una respuesta más detallada se generaría utilizando el modelo LLM real.\"\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_rag(\n",
        "    query: str,\n",
        "    model_name: str = 'all-MiniLM-L6-v2',\n",
        "    top_k: int = 5\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Prueba el funcionamiento del RAG: recupera chunks relevantes y simula una respuesta.\n",
        "\n",
        "    Args:\n",
        "    query (str): La consulta del usuario.\n",
        "    api_key (str): API key de Pinecone.\n",
        "    environment (str): Entorno de Pinecone.\n",
        "    index_name (str): Nombre del índice de Pinecone a utilizar.\n",
        "    model_name (str): Nombre del modelo de SentenceTransformer a utilizar.\n",
        "    top_k (int): Número de chunks más relevantes a recuperar.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Inicializar el modelo de embedding\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    # Crear el embedding de la consulta\n",
        "    query_embedding = model.encode(query).tolist()\n",
        "\n",
        "    # Realizar la búsqueda en Pinecone\n",
        "    search_results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
        "\n",
        "    # Extraer los chunks y sus puntuaciones\n",
        "    retrieved_chunks = [\n",
        "        (result.metadata['text'], result.score) \n",
        "        for result in search_results.matches\n",
        "    ]\n",
        "\n",
        "    print(f\"Consulta: {query}\\n\")\n",
        "    print(\"Chunks recuperados:\")\n",
        "    for i, (chunk, score) in enumerate(retrieved_chunks, 1):\n",
        "        print(f\"\\nChunk {i} (Score: {score:.4f}):\")\n",
        "        print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)\n",
        "\n",
        "    # Simular la generación de respuesta\n",
        "    simulated_response = simulate_llm_response(query, retrieved_chunks)\n",
        "    \n",
        "    print(\"\\nRespuesta simulada del LLM:\")\n",
        "    print(simulated_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consulta: ¿receta queso rawk asqa?\n",
            "\n",
            "Chunks recuperados:\n",
            "\n",
            "Chunk 1 (Score: 0.4487):\n",
            "receta marmitako bonito norte nisqa ingredientes qanchis pachak pichqa chunka gramo musuq bonito kilogramo papa huk clavo ajo huk unidad ch’uñu huk unidad italiano verde pimienta huk kuartal unidad pu...\n",
            "\n",
            "Chunk 2 (Score: 0.4250):\n",
            "receta quri horno ingredientes quri kilo soqta pachak gramo papa kimsa unidad clavokuna ajo huk unidad ch’uñu huk phatma puka pimentu huk phatma verde pimienta huk unidad limón huk cuchara perejil pha...\n",
            "\n",
            "Chunk 3 (Score: 0.4016):\n",
            "receta espárrago tartar salsa ingredientes iskay qutu yuraq espárrago iskay pachak pichqa chunka gramo tartar salsa kimsa litro yaku iskay limón unidad huk cuchara kachi instrucciones ñawpaqta yanapay...\n",
            "\n",
            "Chunk 4 (Score: 0.3995):\n",
            "receta verde habas tomate ingredientes huk kilo verde habas pichqa chunka gramo ch’uñu huk clavo ajo huk kuskan litro tomate salsa huk kuskan vaso aceitunas aceite huk ch’aqchuy yuraq vino huk k’allma...\n",
            "\n",
            "Chunk 5 (Score: 0.3987):\n",
            "receta Ensalada americana ingredientes huk uma k’uyusqa col huk uma romaine lechuga iskay unidad zanahoria huk unidad puka ch’uñu pusaq camarón unidad pusaq unidad cangrejo k’aspikuna iskay unidad sin...\n",
            "\n",
            "Respuesta simulada del LLM:\n",
            "Basándome en la consulta '¿receta queso rawk asqa?' y el contexto proporcionado, puedo decir que la información relevante incluye 5 fragmentos de texto. El contexto total tiene 4083 caracteres. Una respuesta más detallada se generaría utilizando el modelo LLM real.\n"
          ]
        }
      ],
      "source": [
        "test_query = \"¿receta queso rawk asqa?\"\n",
        "test_rag(test_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Using Llama2-7b-HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: peft in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from peft) (24.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from peft) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from peft) (2.3.1)\n",
            "Requirement already satisfied: transformers in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from peft) (4.42.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from peft) (4.66.4)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from peft) (0.32.1)\n",
            "Requirement already satisfied: safetensors in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from peft) (0.4.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from peft) (0.23.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch>=1.13.0->peft) (1.12.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch>=1.13.0->peft) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from transformers->peft) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13.0->peft) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13.0->peft) (2021.13.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: setuptools in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (70.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bitsandbytes in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (0.43.1)\n",
            "Requirement already satisfied: torch in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from bitsandbytes) (2.3.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch->bitsandbytes) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch->bitsandbytes) (1.12.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch->bitsandbytes) (2021.4.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->bitsandbytes) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->bitsandbytes) (2021.13.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (0.32.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from accelerate) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from accelerate) (2.3.1)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (2021.4.0)\n",
            "Requirement already satisfied: requests in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate) (2021.13.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\bruno\\documents\\universidad\\tacc\\tacc-pukyu-yachay\\venv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install peft\n",
        "%pip install --upgrade setuptools\n",
        "%pip install bitsandbytes\n",
        "%pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def improve_query(\n",
        "        query: str, \n",
        "        model, \n",
        "        tokenizer, \n",
        "        max_length: int = 128\n",
        "):\n",
        "    prompt = f\"Mejora la siguiente consulta para buscar información más relevante: '{query}'\\nConsulta mejorada:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1)\n",
        "    \n",
        "    improved_query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return improved_query.split(\"Consulta mejorada:\")[-1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(\n",
        "        query: str, \n",
        "        chunks: list, \n",
        "        model, \n",
        "        tokenizer, \n",
        "):\n",
        "    context = \" \".join(chunks)\n",
        "    prompt = f\"Basado en la siguiente información:\\n\\n{context}\\n\\nResponde a la pregunta: {query}\\n\\nRespuesta:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, num_return_sequences=1, max_new_tokens=300)\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(\"Respuesta:\")[-1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "def load_peft_model(\n",
        "        base_model_path: str, \n",
        "        peft_model_path: str\n",
        "):\n",
        "    # Cargar el modelo base\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    \n",
        "    # Cargar el tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
        "    \n",
        "    # Cargar la configuración PEFT\n",
        "    peft_config = PeftConfig.from_pretrained(peft_model_path)\n",
        "    \n",
        "    # Cargar el modelo PEFT\n",
        "    peft_model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
        "    \n",
        "    return peft_model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rag_with_llama2(\n",
        "    query: str,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    embedding_model,\n",
        "    top_k: int = 5\n",
        "):\n",
        "    # Mejorar la query con Llama 2\n",
        "    improved_query = improve_query(query, model, tokenizer)\n",
        "\n",
        "    # Crear el embedding de la query mejorada\n",
        "    query_embedding = embedding_model.encode(improved_query).tolist()\n",
        "\n",
        "    # Realizar la búsqueda en Pinecone\n",
        "    search_results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
        "\n",
        "    # Extraer los chunks relevantes\n",
        "    relevant_chunks = [result.metadata['text'] for result in search_results.matches]\n",
        "\n",
        "    # Generar respuesta con Llama 2\n",
        "    response = generate_response(improved_query, relevant_chunks, model, tokenizer)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]\n"
          ]
        }
      ],
      "source": [
        "embedding_model_name: str = 'all-MiniLM-L6-v2'\n",
        "base_model_path: str = 'downloaded_model/'\n",
        "llama_model_path: str = 'pretrained_model/results/checkpoint-500/'\n",
        "\n",
        "# Cargar el modelo de embedding\n",
        "embedding_model = SentenceTransformer(embedding_model_name)\n",
        "\n",
        "# Cargar el modelo base\n",
        "model, tokenizer = load_peft_model(base_model_path, llama_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pregunta: ¿Imaynatataq wayk'unchik sumaq papa a la huancaína?\n",
            "Respuesta: “la huancaína es una salsa de origen peruano que se elabora con plátano, fresas y cebolla, acompañada de carne de res y servida con arroz blanco.”\n"
          ]
        }
      ],
      "source": [
        "query = \"¿Imaynatataq wayk'unchik sumaq papa a la huancaína?\"\n",
        "response = rag_with_llama2(query, model, tokenizer, embedding_model)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"Respuesta: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "def unload_model(model):\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Modelo descargado y caché de GPU limpiada.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo descargado y caché de GPU limpiada.\n"
          ]
        }
      ],
      "source": [
        "unload_model(model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ktQmxXNmDVui"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
