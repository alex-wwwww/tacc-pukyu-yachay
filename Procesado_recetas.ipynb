{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-wwwww/tacc-pukyu-yachay/blob/master/ProyectoCorpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-sc7Nu14tzMN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/nhrot/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/nhrot/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/nhrot/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /home/nhrot/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transformar_numero_cadena(cadena):\n",
        "    numeros = {\n",
        "        '0': 'cero', '1': 'uno', '2': 'dos', '3': 'tres', '4': 'cuatro', '5': 'cinco', '6': 'seis', '7': 'siete', '8': 'ocho', '9': 'nueve',\n",
        "        '10': 'diez', '11': 'once', '12': 'doce', '13': 'trece', '14': 'catorce', '15': 'quince', '16': 'dieciséis', '17': 'diecisiete', '18': 'dieciocho', '19': 'diecinueve',\n",
        "        '20': 'veinte', '21': 'veintiuno', '22': 'veintidós', '23': 'veintitrés', '24': 'veinticuatro', '25': 'veinticinco', '26': 'veintiséis', '27': 'veintisiete', '28': 'veintiocho', '29': 'veintinueve',\n",
        "        '30': 'treinta', '31': 'treinta y uno', '32': 'treinta y dos', '33': 'treinta y tres', '34': 'treinta y cuatro', '35': 'treinta y cinco', '36': 'treinta y seis', '37': 'treinta y siete', '38': 'treinta y ocho', '39': 'treinta y nueve',\n",
        "        '40': 'cuarenta', '45': 'cuarenta y cinco', '50': 'cincuenta', '55': 'cincuenta y cinco', '60': 'sesenta', '65': 'sesenta y cinco', '70': 'setenta', '75': 'setenta y cinco', '80': 'ochenta', '85': 'ochenta y cinco', '90': 'noventa',\n",
        "        '100': 'cien', '150': 'ciento cincuenta', '200': 'doscientos', '250': 'doscientos cincuenta', '300': 'trescientos', '350': 'trescientos cincuenta', '400': 'cuatrocientos', '450': 'cuatrocientos cincuenta', '500': 'quinientos', '550': 'quinientos cincuenta', '600': 'seiscientos', '650': 'seiscientos cincuenta', '700': 'setecientos', '750': 'setecientos cincuenta', '800': 'ochocientos', '850': 'ochocientos cincuenta', '900': 'novecientos',\n",
        "        '1.': 'primero', '2.': 'segundo', '3.': 'tercero', '4.': 'cuarto', '5.': 'quinto', '6.': 'sexto', '7.': 'séptimo', '8.': 'octavo', '9.': 'noveno',\n",
        "        'º.-': 'primer lugar', '2º.-': 'en segundo lugar', '3º.-': 'en tercer lugar', '4º.-': 'en cuarto lugar', '5º.-': 'en quinto lugar', '6º.-': 'en sexto lugar', '7º.-': 'en séptimo lugar', '8º.-': 'en octavo lugar', '9º.-': 'en noveno lugar',\n",
        "        '/2': 'medio', '/4': 'cuarto', '/8': 'octavo', '/3': 'tercio'\n",
        "        }\n",
        "    \n",
        "    cadena = cadena.lower()\n",
        "    cadena = ' '.join(cadena.split())\n",
        "    \n",
        "    words = []\n",
        "    for word in cadena.split():\n",
        "        temp = word.split('/')\n",
        "        if len(temp) > 1:\n",
        "            if temp[0] in numeros.keys() and temp[1] in numeros.keys():\n",
        "                words.append(numeros[temp[0]] + ' ' + numeros['/' + temp[1]])\n",
        "        else:\n",
        "            words.append(word)\n",
        "    \n",
        "    words = [ x if x not in numeros.keys() else numeros[x] for x in words ]\n",
        "    cadena = ' '.join(words)\n",
        "    \n",
        "    words = []\n",
        "    for word in cadena.split():\n",
        "        temp = word.split('1')\n",
        "        if len(temp) > 1:\n",
        "            if temp[0] in numeros.keys() and temp[1] == '°.-':\n",
        "                words.append(numeros[temp[0]] + ' ' + numeros[temp[1]])\n",
        "        else:\n",
        "            words.append(word)\n",
        "    \n",
        "    words = [ x if x not in numeros.keys() else numeros[x] for x in words ]\n",
        "    cadena = ' '.join(words)\n",
        "    return cadena  \n",
        "\n",
        "def f_preprocesamiento(cadena):\n",
        "  # Pasar a minusculas\n",
        "  cadena = transformar_numero_cadena(cadena)\n",
        "  # Eliminar espacios sobrantes\n",
        "  cadena = ' '.join(cadena.split())\n",
        "  # Tokenizar (No se solicita, pero se facilita procesamiento posterior)\n",
        "  words = []\n",
        "  words = cadena.split()\n",
        "  cadena = ' '.join(words)\n",
        "  words = []\n",
        "  for sentence in sent_tokenize(cadena):\n",
        "    words.append(word_tokenize(sentence))\n",
        "  words = [x for l in words for x in l]\n",
        "  # Procesar contracciones\n",
        "  contract_dict = {'ud': 'unidad', 'kg': 'kilogramo', 'gr': 'gramo', 'ml': 'mililitro', 'dl': 'decilitro',\n",
        "                    'cm': 'centímetro', 'mm': 'milímetro', 'm': 'metro', 'km': 'kilómetro', 'l': 'litro',\n",
        "                    'seg': 'segundo', 'min': 'minuto', 'hr': 'hora', 'dia': 'día', 'mes': 'mes', 'año': 'año',\n",
        "                   }\n",
        "  words = [contract_dict[y] if y in contract_dict.keys() else y for y in words]\n",
        "  # Eliminar grafías\n",
        "  tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "  words = tokenizer.tokenize(' '.join(words))\n",
        "  \n",
        "  # Eliminar stopwords\n",
        "  en_stopwords = stopwords.words('spanish')\n",
        "  words =  [y for y in words if y not in en_stopwords or y == 'uno']\n",
        "  \n",
        "  cadena = ' '.join(words)\n",
        "  cadena = transformar_numero_cadena(cadena)\n",
        "  # La función debe devolver un string.\n",
        "  return cadena"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def procesar_texto(text_file):\n",
        "    lineas = []\n",
        "    with open(text_file, 'r') as file:\n",
        "        for line in file:\n",
        "            lineas.append(line)\n",
        "    \n",
        "    cadena = \"\"\n",
        "    for linea in lineas:\n",
        "        # add end of line\n",
        "        cadena += f_preprocesamiento(linea) + '\\n'\n",
        "                \n",
        "    return cadena"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "receta albóndigas tomate\n",
            "ingredientes\n",
            "doscientos cincuenta gramo carne ternera picada\n",
            "doscientos cincuenta gramo carne cerdo picada\n",
            "cuatro rebanada pan molde\n",
            "uno unidad huevo\n",
            "uno chorro leche\n",
            "uno chorro vino blanco\n",
            "uno diente ajo\n",
            "uno cucharada perejil picado\n",
            "uno pizca pimienta blanca molida\n",
            "uno cucharada sal\n",
            "cien gramo harina\n",
            "uno litro salsa tomate\n",
            "instrucciones\n",
            "primero pon rebanadas pan remojo buen chorro leche diez minutos\n",
            "segundo mezcla carnes sazona sal pimienta aromatiza vino diente ajo picado brunoisecorte hortalizas verduras dados finos regulares\n",
            "tercero añade pan remojado leche perejil picado huevo crudo amasa minutos\n",
            "cuarto forma porciones menos veinticinco gramo dales forma bola puedes utilizar harina evitar peguen\n",
            "quinto vez albóndigas formadas enharínalas fríelas abundante aceite caliente uno dos minutos doradas\n",
            "sexto colócalas olla cazuela añade salsa tomate cocínalas fuego lento cuarenta minutos aproximadamente si necesario puedes añadir vaso agua ó caldo evitar salsa quede espesa\n",
            "séptimo acompaña patatas fritas cortadas dados guisantes cocidos\n",
            "\n"
          ]
        }
      ],
      "source": [
        "project_root = os.getcwd()\n",
        "recetas_dir = os.path.join(project_root, 'recetas/recetas')\n",
        "recetas_dir2 = os.path.join(project_root, 'recetas2')\n",
        "\n",
        "recetas_files = os.listdir(recetas_dir)\n",
        "recetas_files.sort()\n",
        "recetas_files2 = os.listdir(recetas_dir2)\n",
        "recetas_files2.sort()\n",
        "\n",
        "print(procesar_texto(os.path.join(recetas_dir, recetas_files[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exportando recetas procesadas\n",
        "recetas_procesadas_dir = os.path.join(project_root, 'recetas_procesadas')\n",
        "os.makedirs(recetas_procesadas_dir, exist_ok=True)\n",
        "\n",
        "# Limpiando directorio\n",
        "for file in os.listdir(recetas_procesadas_dir):\n",
        "    os.remove(os.path.join(recetas_procesadas_dir, file))\n",
        "\n",
        "# Agregar recetas procesadas\n",
        "for file in recetas_files:\n",
        "    with open(os.path.join(recetas_procesadas_dir, file), 'w') as file_procesado:\n",
        "        # print(file)\n",
        "        file_procesado.write(procesar_texto(os.path.join(recetas_dir, file)))\n",
        "\n",
        "for file in recetas_files2:\n",
        "    with open(os.path.join(recetas_procesadas_dir, file), 'w') as file_procesado:\n",
        "        # print(file)\n",
        "        file_procesado.write(procesar_texto(os.path.join(recetas_dir2, file)))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOPSUstmvkIL5xszIs/mHj2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
